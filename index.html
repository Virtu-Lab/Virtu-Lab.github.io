<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta name="theme-color" content="#000000" />
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="Open-Sans.css">
  <link rel="stylesheet" href="index.css">
  <title></title>
  <script defer="defer" src="./static/js/main.cb41f6a5.js"></script>
  <link href="./static/css/main.4017e162.css" rel="stylesheet">
  <meta name="description"
        content="OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models">
  <title>OmniHuman-1 Project</title>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://github.com/bytedance/DreamFit/">
                        DreamFit
                    </a>
                </div>
            </div>
        </div>

    </div>
  </nav>

  <div id="root" class="column-flex">
    <div id="title-flex" class="column-flex">
      <h1> Try-On Master: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-wise Diffusion Transformer Framework </h1>
      <span>
        <a target="_blank" href="" onclick="return false;" >Tongchun&nbsp;Zuo</a><sup>*</sup>,
        <a target="_blank" href="" onclick="return false;" >Zaiyu&nbsp;Huang</a><sup>*</sup>,
        <a  href="https://ningshuliang.github.io/" >Shuliang&nbsp;Ning</a>,
        <a target="_blank" href="" onclick="return false;" >Xin&nbsp;Dong</a><sup>†</sup>
        
        <br />
      </span>
      <span>Bytedance</span>
      <span><sup>*</sup>Equal contribution,<sup>†</sup>Project lead
        <!-- ,<sup>‡</sup>Internship at Bytedance -->
      </span>
      <div class="flex flex-gap" style="margin-bottom:0.5em;">
        <a target="_blank" href="http://arxiv.org/abs/2502.01061" ><button>Paper</button></a>
<!-- 	      <a target="_blank" href="" onclick="alert('Coming Soon!');return false;"><button>Paper</button></a> -->
        <a target="_blank" href="https://omnihuman-lab.github.io"><button>Page</button></a>
      </div>

      <!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video virtual try-on (VVT) in real-world environments aims to realistically fit garments onto human subjects in highly demanding scenarios, including unrestricted subject or camera movement, dynamic scenes, and character styles. However, current approaches face two primary challenges: First, these methods heavily rely on paired clothing-video or garment-image data, which are scarce across diverse settings. This scarcity often leads to garment appearance degradation or temporal jitter, especially in intricate situations. Second, a substantial gap exists between the task of fitting spatially misaligned garment images across video frames and the spatiotemporal consistency priors of pretrained video models, making the convergence of these approaches challenging. To overcome these issues, we present 
            <b>Try-On Master</b>, a stage-wise framework based on Diffusion Transformers (DiTs) that decomposes the VVT task into keyframe image try-on and video generation guided by spatially aligned keyframes, thereby narrowing the gap with pretrained priors. This design also enables training on large-scale unpaired video and image datasets from diverse sources. Specifically, the framework comprises three sequential stages. In the first stage, we propose a <b>keyframe sampling strategy</b> that selects frames exhibiting significant motion changes in complex scenarios, thereby providing comprehensive reference cues for subsequent video generation. In the second stage, we develop a <b>multi-frame try-on model</b> that fits garments to keyframes while accurately preserving garment details and ensuring content consistency. In the third stage, we introduce a <b>multi-modal guided video editing model</b> that synthesizes temporally stable virtual try-on videos using keyframe try-on images, motion features, and textual descriptions as guidance. Extensive experiments demonstrate that Try-On Master surpasses existing methods on both image and video try-on tasks, particularly in preserving high-fidelity garment details and maintaining temporal stability under real-world conditions. Video samples are available on our project homepage: [insert URL here].
          </p>

        </div>
      </div>
    </div>
  </div>
</section>


        <!-- <b>TL;DR</b>: We propose an end-to-end multimodality-conditioned human video generation framework named OmniHuman, which can generate human videos based on a single human image and motion signals (e.g., audio only, video only, or a combination of audio and video). In OmniHuman, we introduce a multimodality motion conditioning mixed training strategy, allowing the model to benefit from data scaling up of mixed conditioning. This overcomes the issue that previous end-to-end approaches faced due to the scarcity of high-quality data. OmniHuman significantly outperforms existing methods, generating extremely realistic human videos based on weak signal inputs, especially audio. It supports image inputs of any aspect ratio, whether they are portraits, half-body, or full-body images, delivering more lifelike and high-quality results across various scenarios.</span></small> -->
      <!-- <small><span><b>Currently, we do not offer services/downloads anywhere, nor do we have any SNS accounts for the project.</b></span></small>
      <small><span><b>Please be cautious of fraudulent information. We will provide timely updates on future developments.</b></span></small> -->
      <div class='responsive-image-container'>
        <img src='image/framework.png' alt='' />
      </div>
    </div>

    <div id="sections" class="column-flex">
      
      <h3><h2 class="title is-3">High-fidelity garment detail preservation under complex motion</h2></h3>
        <p class="styled-text">
          <b>*</b> Try-On Master enables virtual try-on of complete outfits—including tops, bottoms, skirts, shoes, socks, and more. If a user uploads only a top, the model can automatically generate and match appropriate bottoms and footwear to complete the outfit. This capability is not available in previous methods.
        
        </p>
         <p class="styled-text">
          <b>*</b> Try-On Master is capable of handling complex human motions, including runway walks and 360-degree rotations, with high fidelity in garment detail preservation and robust temporal consistency.
        
        </p>
        <div class="video-slider">
          <video src="video/main1.mp4"></video>
          <video src="video/main2.mp4"></video>
          <video src="video/main3.mp4"></video>
          <video src="video/main4.mp4"></video>
        </div>
        <!-- <div class="video-slider">
          <video src="video/talk2.mp4"></video>
          <video src="video/talk3.mp4"></video>
        </div> -->
        
        <!-- 
      <h3>Talking</h3>
        <p>OmniHuman can support input of any aspect ratio in terms of speech. It significantly improves the handling of gestures, which is a challenge for existing methods, and produces highly realistic results. The audio and images for some of the test cases are sourced from <a href="https://www.youtube.com/watch?v=5Jk8qITsqdM&t=127s&ab_channel=TEDxTalks">link1</a>, <a href="https://www.youtube.com/watch?v=ITxWUu6UcWQ&t=251s&ab_channel=TEDxTalks">link2</a>, <a href="https://www.youtube.com/watch?v=oO8w6XcXJUs&ab_channel=RealTimewithBillMaher">link3</a>, <a href="https://www.youtube.com/watch?v=oO8w6XcXJUs&ab_channel=RealTimewithBillMaher">link4</a>.</p>
        <div class="video-slider">
          <video src="video/talk2.mp4"></video>
          <video src="video/talk3.mp4"></video>
        </div>
        
 -->

      <h3><h2 class="title is-3">High-fidelity garment rendering under challenging camera dynamics</h2></h3>

         <p class="styled-text">
          <b>*</b> Try-On Master can preserve temporal consistency and high-fidelity garment details, even when the input video features challenging camera movements and prominent scene transitions. 
        </p>
        <div class="video-slider">
          <video src="video/div1.mp4"></video>
          <video src="video/div2.mp4"></video>
        </div>


      <h3><h2 class="title is-3">Generating plausible physical dynamics during garment interactions</h2></h3>

         <p class="styled-text">
          <b>*</b> Try-On Master can generate realistic physical dynamics in scenarios involving garment interactions, for example, inserting hands into pockets or interacting with soft clothing materials.
        </p>
        <div class="video-slider">
          <video src="video/gen1.mp4"></video>
          <video src="video/gen2.mp4"></video>
        </div>

       
      <h3><h2 class="title is-3">High-fidelity garment detail preservation in challenging scenarios</h2></h3>

         <p class="styled-text">
          <b>*</b> Try-On Master is capable of enabling virtual try-on in videos featuring subjects within complex static or dynamic environments.
        </p>
        <div class="video-slider">
          <video src="video/challenge1.mp4"></video>
        </div>
      

      <h3><h2 class="title is-3">Try-on Show Time</h2></h3>
        <div class="video-slider">
          <video src="video/show1.MP4"></video>
          <video src="video/show2.MP4"></video>
          <video src="video/show3.MP4"></video>
        </div>

         <div class="video-slider">
          <video src="video/show4.MP4"></video>
          <video src="video/show5.MP4"></video>
          <video src="video/show6.MP4"></video>
        </div>

         <div class="video-slider">
          <video src="video/show7.MP4"></video>
          <video src="video/show8.MP4"></video>
          <video src="video/show9.MP4"></video>
        </div>

      

      <h3><h2 class="title is-3">Outfitting cartoon characters in highly demanding scenarios</h2></h3>

         <p class="styled-text">
          <b>*</b> Even more interestingly, Try-On Master is capable of outfitting cartoon characters with real-world garments, even in highly demanding scenarios involving unrestricted subject poses or camera movement and dynamic scenes.       
         </p>
        <div class="video-slider">
          <video src="video/cartoon1.mp4"></video>
          <video src="video/cartoon2.mp4"></video>
          <video src="video/cartoon3.mp4"></video>
          <video src="video/cartoon4.mp4"></video>
        </div>

        

      <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ethics Concerns</h2>
        <div class="content has-text-justified">
          <p>
                      The images and audios used in these demos are from public sources or generated by models, and are solely used to demonstrate the capabilities of this research work. If there are any concerns, please contact us (dongxin.1016@bytedance.com) and we will delete it in time.
         </p>

        </div>
      </div>
    </div>
  </div>
</section>




      




      <!-- <h3>Compatibility with Video Driving</h3>
        <p>Due to OmniHuman's mixed condition training characteristics, it can support not only audio driving but also video driving to mimic specific video actions, as well as combined audio and video driving to control specific body parts like recent methods.



      <h3>Ethics Concerns</h3>
        <p>
          The images and audios used in these demos are from public sources or generated by models, and are solely used to demonstrate the capabilities of this research work. If there are any concerns, please contact us (jianwen.alan@gmail.com) and we will delete it in time. The template of this webpage is based on the one from <a href="https://www.microsoft.com/en-us/research/project/vasa-1/">VASA-1</a>,  and some test audios are from <a href="https://www.microsoft.com/en-us/research/project/vasa-1">VASA-1</a>,<a href="https://loopyavatar.github.io">Loopy</a>,<a href="https://cyberhost.github.io/">CyberHost</a>.
        </p> -->
      
      <!-- <h3>BibTeX</h3>
        <p>If you find this project useful for your research, you can cite us and check out our other related works:</p>
        <pre><code>
          @article{lin2025omnihuman1,
            title={OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models}, 
            author={Gaojie Lin and Jianwen Jiang and Jiaqi Yang and Zerong Zheng and Chao Liang},
            journal={arXiv preprint arXiv:2502.01061},
            year={2025}
          }
          

        </code></pre>  -->

      <br/>
      <br/>
      <br/>
    </div>
  </div>
  <script src="index.js"></script>
  <script>
    function comming_soon_click() {
      alert('Comming soon!');
    }
    function TBD_click() {
      alert('TBD');
    }
  </script>
</body>



</html>
